# Scripts Directory

This directory contains various scripts used for data processing and result combination in the NLP group project. Below is a description of each script and its functionality.

## Scripts

### 1. `combine_result.py`

- **Description**: Combines results generated by different methods and saves them in the `result` directory.
- **Functionality**:
  - Reads `.arrow` files from different method directories under the `output` directory.
  - Combines the data into a single dataset while preserving the `abstract` and adding columns for each method's output.
  - Saves the combined dataset in the `results` directory.

### 2. `data_cleaner_ArXiv.py`

- **Description**: Cleans the ArXiv dataset located in the `data/origin/ArXiv` directory.
- **Functionality**:
  - Removes HTML tags, LaTeX commands, placeholders, and other special symbols from the text.
  - Filters out entries where the abstract length exceeds 3000 characters.
  - Converts the cleaned data into `.arrow` format and saves it in the `data/processed/ArXiv` directory.

### 3. `data_cleaner_CNN.py`

- **Description**: Cleans the CNN/Daily Mail dataset located in the `data/origin/cnn_dailymail` directory.
- **Functionality**:
  - Removes HTML tags and special symbols from the text.
  - Removes location and author information from the beginning of articles.
  - Filters out entries where the highlights length exceeds 752 characters.
  - Converts the cleaned data into `.arrow` format and saves it in the `data/processed/cnn_dailymail` directory.

### 4. `read_data_for_test.ipynb` and `test.ipynb`

- **Description**: Jupyter notebooks used for reading `.arrow` files and testing functionalities.
- **Functionality**:
  - Provide examples of how to read and process the `.arrow` files.
  - Include test cases to verify the data cleaning and combination processes.

## Directory Structure

- `combine_result.py`
- `data_cleaner_ArXiv.py`
- `data_cleaner_CNN.py`
- `read_data_for_test.ipynb`
- `test.ipynb`

## Usage

1. **Data Cleaning**:
   - Run `data_cleaner_ArXiv.py` to clean the ArXiv dataset.
   - Run `data_cleaner_CNN.py` to clean the CNN/Daily Mail dataset.

2. **Result Combination**:
   - Run `combine_result.py` to combine the results of different methods.

3. **Testing**:
   - Use `read_data_for_test.ipynb` and `test.ipynb` to test and validate the data processing steps.


# 脚本目录

该目录包含用于自然语言处理小组项目的数据处理和结果合并的各种脚本。以下是每个脚本及其功能的描述。

## 脚本

### 1. `combine_result.py`

- **描述**：合并不同方法生成的结果并将其保存到 `result` 目录。
- **功能**：
  - 从 `output` 目录下的不同方法目录中读取 `.arrow` 文件。
  - 将数据合并为一个数据集，同时保留 `abstract` 并为每个方法的输出添加列。
  - 将合并后的数据集保存到 `results` 目录。

### 2. `data_cleaner_ArXiv.py`

- **描述**：清理位于 `data/origin/ArXiv` 目录下的 ArXiv 数据集。
- **功能**：
  - 去除文本中的 HTML 标签、LaTeX 命令、占位符和其他特殊符号。
  - 过滤掉摘要长度超过 3000 字符的条目。
  - 将清理后的数据转换为 `.arrow` 格式并保存到 `data/processed/ArXiv` 目录。

### 3. `data_cleaner_CNN.py`

- **描述**：清理位于 `data/origin/cnn_dailymail` 目录下的 CNN/Daily Mail 数据集。
- **功能**：
  - 去除文本中的 HTML 标签和特殊符号。
  - 去除文章开头的位置信息和作者信息。
  - 过滤掉摘要长度超过 752 字符的条目。
  - 将清理后的数据转换为 `.arrow` 格式并保存到 `data/processed/cnn_dailymail` 目录。

### 4. `read_data_for_test.ipynb` 和 `test.ipynb`

- **描述**：用于读取 `.arrow` 文件和测试功能的 Jupyter 笔记本。
- **功能**：
  - 提供如何读取和处理 `.arrow` 文件的示例。
  - 包含验证数据清理和合并过程的测试用例。

## 目录结构

- `combine_result.py`
- `data_cleaner_ArXiv.py`
- `data_cleaner_CNN.py`
- `read_data_for_test.ipynb`
- `test.ipynb`

## 使用说明

1. **数据清理**：
   - 运行 `data_cleaner_ArXiv.py` 清理 ArXiv 数据集。
   - 运行 `data_cleaner_CNN.py` 清理 CNN/Daily Mail 数据集。

2. **结果合并**：
   - 运行 `combine_result.py` 合并不同方法的结果。

3. **测试**：
   - 使用 `read_data_for_test.ipynb` 和 `test.ipynb` 测试和验证数据处理步骤。
